{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOLIZtrBsQGK"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMgniBIisQGN"
      },
      "source": [
        "# Introduction to deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tijv4_jDsQGO"
      },
      "source": [
        "## Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-kvEazqsQGO"
      },
      "source": [
        "**Instantiating a small convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "woJDkjvNsQGO"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shPw-Aj5sQGP"
      },
      "source": [
        "**Displaying the model's summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "4ih8eQVvsQGQ",
        "outputId": "fb855d24-ba00-4efd-ced4-f5c3c358727c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEs6Rf-UsQGR"
      },
      "source": [
        "**Training the convnet on MNIST images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rCI284vOsQGR",
        "outputId": "de26ca24-cfc0-4e79-8acb-4db3024ca777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 62s 65ms/step - loss: 0.1523 - accuracy: 0.9535\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 70s 74ms/step - loss: 0.0439 - accuracy: 0.9863\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 64s 68ms/step - loss: 0.0318 - accuracy: 0.9903\n",
            "Epoch 4/5\n",
            "382/938 [===========>..................] - ETA: 37s - loss: 0.0222 - accuracy: 0.9929"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3637a4c016e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     metrics=[\"accuracy\"])\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8t17CpsQGS"
      },
      "source": [
        "**Evaluating the convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NBIBVRSsQGS"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EnEzHfBsQGS"
      },
      "source": [
        "### The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmlp73zUsQGT"
      },
      "source": [
        "#### Understanding border effects and padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3AaMXPsQGT"
      },
      "source": [
        "#### Understanding convolution strides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogWe71f9sQGT"
      },
      "source": [
        "### The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbynOHdbsQGT"
      },
      "source": [
        "**An incorrectly structured convnet missing its max-pooling layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSYRdlI6sQGU"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEqqN3I1sQGU"
      },
      "outputs": [],
      "source": [
        "model_no_max_pool.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnbx42j4sQGU"
      },
      "source": [
        "## Training a convnet from scratch on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58mAx2usQGU"
      },
      "source": [
        "### The relevance of deep learning for small-data problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAy7nKmzsQGV"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWs78zlnsQGV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tq_0BvDsQGV"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eLcRii_sQGV"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPkVp4_NsQGV"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVSMtrvRsQGW"
      },
      "source": [
        "**Copying images to training, validation, and test directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "xJm7zoaUsQGW",
        "outputId": "bd2fa333-3df0-47ea-bea9-894b45ba124e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog.8.jpgnot found.\n",
            "dog.9.jpgnot found.\n",
            "dog.72.jpgnot found.\n",
            "dog.73.jpgnot found.\n",
            "dog.74.jpgnot found.\n",
            "dog.75.jpgnot found.\n",
            "dog.76.jpgnot found.\n",
            "dog.77.jpgnot found.\n",
            "dog.78.jpgnot found.\n",
            "dog.79.jpgnot found.\n",
            "dog.80.jpgnot found.\n",
            "dog.81.jpgnot found.\n",
            "dog.82.jpgnot found.\n",
            "dog.83.jpgnot found.\n",
            "dog.84.jpgnot found.\n",
            "dog.85.jpgnot found.\n",
            "dog.86.jpgnot found.\n",
            "dog.87.jpgnot found.\n",
            "dog.88.jpgnot found.\n",
            "dog.89.jpgnot found.\n",
            "dog.90.jpgnot found.\n",
            "dog.91.jpgnot found.\n",
            "dog.92.jpgnot found.\n",
            "dog.93.jpgnot found.\n",
            "dog.94.jpgnot found.\n",
            "dog.95.jpgnot found.\n",
            "dog.96.jpgnot found.\n",
            "dog.97.jpgnot found.\n",
            "dog.98.jpgnot found.\n",
            "dog.99.jpgnot found.\n",
            "dog.716.jpgnot found.\n",
            "dog.717.jpgnot found.\n",
            "dog.718.jpgnot found.\n",
            "dog.719.jpgnot found.\n",
            "dog.720.jpgnot found.\n",
            "dog.721.jpgnot found.\n",
            "dog.722.jpgnot found.\n",
            "dog.723.jpgnot found.\n",
            "dog.724.jpgnot found.\n",
            "dog.725.jpgnot found.\n",
            "dog.726.jpgnot found.\n",
            "dog.727.jpgnot found.\n",
            "dog.728.jpgnot found.\n",
            "dog.729.jpgnot found.\n",
            "dog.730.jpgnot found.\n",
            "dog.731.jpgnot found.\n",
            "dog.732.jpgnot found.\n",
            "dog.733.jpgnot found.\n",
            "dog.734.jpgnot found.\n",
            "dog.735.jpgnot found.\n",
            "dog.736.jpgnot found.\n",
            "dog.737.jpgnot found.\n",
            "dog.738.jpgnot found.\n",
            "dog.739.jpgnot found.\n",
            "dog.740.jpgnot found.\n",
            "dog.741.jpgnot found.\n",
            "dog.742.jpgnot found.\n",
            "dog.743.jpgnot found.\n",
            "dog.744.jpgnot found.\n",
            "dog.745.jpgnot found.\n",
            "dog.746.jpgnot found.\n",
            "dog.747.jpgnot found.\n",
            "dog.748.jpgnot found.\n",
            "dog.749.jpgnot found.\n",
            "dog.750.jpgnot found.\n",
            "dog.751.jpgnot found.\n",
            "dog.752.jpgnot found.\n",
            "dog.753.jpgnot found.\n",
            "dog.754.jpgnot found.\n",
            "dog.755.jpgnot found.\n",
            "dog.756.jpgnot found.\n",
            "dog.757.jpgnot found.\n",
            "dog.758.jpgnot found.\n",
            "dog.759.jpgnot found.\n",
            "dog.760.jpgnot found.\n",
            "dog.761.jpgnot found.\n",
            "dog.762.jpgnot found.\n",
            "dog.763.jpgnot found.\n",
            "dog.764.jpgnot found.\n",
            "dog.765.jpgnot found.\n",
            "dog.766.jpgnot found.\n",
            "dog.767.jpgnot found.\n",
            "dog.768.jpgnot found.\n",
            "dog.769.jpgnot found.\n",
            "dog.770.jpgnot found.\n",
            "dog.771.jpgnot found.\n",
            "dog.772.jpgnot found.\n",
            "dog.773.jpgnot found.\n",
            "dog.774.jpgnot found.\n",
            "dog.775.jpgnot found.\n",
            "dog.776.jpgnot found.\n",
            "dog.777.jpgnot found.\n",
            "dog.778.jpgnot found.\n",
            "dog.779.jpgnot found.\n",
            "dog.780.jpgnot found.\n",
            "dog.781.jpgnot found.\n",
            "dog.782.jpgnot found.\n",
            "dog.783.jpgnot found.\n",
            "dog.784.jpgnot found.\n",
            "dog.785.jpgnot found.\n",
            "dog.786.jpgnot found.\n",
            "dog.787.jpgnot found.\n",
            "dog.788.jpgnot found.\n",
            "dog.789.jpgnot found.\n",
            "dog.790.jpgnot found.\n",
            "dog.791.jpgnot found.\n",
            "dog.792.jpgnot found.\n",
            "dog.793.jpgnot found.\n",
            "dog.794.jpgnot found.\n",
            "dog.795.jpgnot found.\n",
            "dog.796.jpgnot found.\n",
            "dog.797.jpgnot found.\n",
            "dog.798.jpgnot found.\n",
            "dog.799.jpgnot found.\n",
            "dog.800.jpgnot found.\n",
            "dog.801.jpgnot found.\n",
            "dog.802.jpgnot found.\n",
            "dog.803.jpgnot found.\n",
            "dog.804.jpgnot found.\n",
            "dog.805.jpgnot found.\n",
            "dog.806.jpgnot found.\n",
            "dog.807.jpgnot found.\n",
            "dog.808.jpgnot found.\n",
            "dog.809.jpgnot found.\n",
            "dog.810.jpgnot found.\n",
            "dog.811.jpgnot found.\n",
            "dog.812.jpgnot found.\n",
            "dog.813.jpgnot found.\n",
            "dog.814.jpgnot found.\n",
            "dog.815.jpgnot found.\n",
            "dog.816.jpgnot found.\n",
            "dog.817.jpgnot found.\n",
            "dog.818.jpgnot found.\n",
            "dog.819.jpgnot found.\n",
            "dog.820.jpgnot found.\n",
            "dog.821.jpgnot found.\n",
            "dog.822.jpgnot found.\n",
            "dog.823.jpgnot found.\n",
            "dog.824.jpgnot found.\n",
            "dog.825.jpgnot found.\n",
            "dog.826.jpgnot found.\n",
            "dog.827.jpgnot found.\n",
            "dog.828.jpgnot found.\n",
            "dog.829.jpgnot found.\n",
            "dog.830.jpgnot found.\n",
            "dog.831.jpgnot found.\n",
            "dog.832.jpgnot found.\n",
            "dog.833.jpgnot found.\n",
            "dog.834.jpgnot found.\n",
            "dog.835.jpgnot found.\n",
            "dog.836.jpgnot found.\n",
            "dog.837.jpgnot found.\n",
            "dog.838.jpgnot found.\n",
            "dog.839.jpgnot found.\n",
            "dog.840.jpgnot found.\n",
            "dog.841.jpgnot found.\n",
            "dog.842.jpgnot found.\n",
            "dog.843.jpgnot found.\n",
            "dog.844.jpgnot found.\n",
            "dog.845.jpgnot found.\n",
            "dog.846.jpgnot found.\n",
            "dog.847.jpgnot found.\n",
            "dog.848.jpgnot found.\n",
            "dog.849.jpgnot found.\n",
            "dog.850.jpgnot found.\n",
            "dog.851.jpgnot found.\n",
            "dog.852.jpgnot found.\n",
            "dog.853.jpgnot found.\n",
            "dog.854.jpgnot found.\n",
            "dog.855.jpgnot found.\n",
            "dog.856.jpgnot found.\n",
            "dog.857.jpgnot found.\n",
            "dog.858.jpgnot found.\n",
            "dog.859.jpgnot found.\n",
            "dog.860.jpgnot found.\n",
            "dog.861.jpgnot found.\n",
            "dog.862.jpgnot found.\n",
            "dog.863.jpgnot found.\n",
            "dog.864.jpgnot found.\n",
            "dog.865.jpgnot found.\n",
            "dog.866.jpgnot found.\n",
            "dog.867.jpgnot found.\n",
            "dog.868.jpgnot found.\n",
            "dog.869.jpgnot found.\n",
            "dog.870.jpgnot found.\n",
            "dog.871.jpgnot found.\n",
            "dog.872.jpgnot found.\n",
            "dog.873.jpgnot found.\n",
            "dog.874.jpgnot found.\n",
            "dog.875.jpgnot found.\n",
            "dog.876.jpgnot found.\n",
            "dog.877.jpgnot found.\n",
            "dog.878.jpgnot found.\n",
            "dog.879.jpgnot found.\n",
            "dog.880.jpgnot found.\n",
            "dog.881.jpgnot found.\n",
            "dog.882.jpgnot found.\n",
            "dog.883.jpgnot found.\n",
            "dog.884.jpgnot found.\n",
            "dog.885.jpgnot found.\n",
            "dog.886.jpgnot found.\n",
            "dog.887.jpgnot found.\n",
            "dog.888.jpgnot found.\n",
            "dog.889.jpgnot found.\n",
            "dog.890.jpgnot found.\n",
            "dog.891.jpgnot found.\n",
            "dog.892.jpgnot found.\n",
            "dog.893.jpgnot found.\n",
            "dog.894.jpgnot found.\n",
            "dog.895.jpgnot found.\n",
            "dog.896.jpgnot found.\n",
            "dog.897.jpgnot found.\n",
            "dog.898.jpgnot found.\n",
            "dog.899.jpgnot found.\n",
            "dog.900.jpgnot found.\n",
            "dog.901.jpgnot found.\n",
            "dog.902.jpgnot found.\n",
            "dog.903.jpgnot found.\n",
            "dog.904.jpgnot found.\n",
            "dog.905.jpgnot found.\n",
            "dog.906.jpgnot found.\n",
            "dog.907.jpgnot found.\n",
            "dog.908.jpgnot found.\n",
            "dog.909.jpgnot found.\n",
            "dog.910.jpgnot found.\n",
            "dog.911.jpgnot found.\n",
            "dog.912.jpgnot found.\n",
            "dog.913.jpgnot found.\n",
            "dog.914.jpgnot found.\n",
            "dog.915.jpgnot found.\n",
            "dog.916.jpgnot found.\n",
            "dog.917.jpgnot found.\n",
            "dog.918.jpgnot found.\n",
            "dog.919.jpgnot found.\n",
            "dog.920.jpgnot found.\n",
            "dog.921.jpgnot found.\n",
            "dog.922.jpgnot found.\n",
            "dog.923.jpgnot found.\n",
            "dog.924.jpgnot found.\n",
            "dog.925.jpgnot found.\n",
            "dog.926.jpgnot found.\n",
            "dog.927.jpgnot found.\n",
            "dog.928.jpgnot found.\n",
            "dog.929.jpgnot found.\n",
            "dog.930.jpgnot found.\n",
            "dog.931.jpgnot found.\n",
            "dog.932.jpgnot found.\n",
            "dog.933.jpgnot found.\n",
            "dog.934.jpgnot found.\n",
            "dog.935.jpgnot found.\n",
            "dog.936.jpgnot found.\n",
            "dog.937.jpgnot found.\n",
            "dog.938.jpgnot found.\n",
            "dog.939.jpgnot found.\n",
            "dog.940.jpgnot found.\n",
            "dog.941.jpgnot found.\n",
            "dog.942.jpgnot found.\n",
            "dog.943.jpgnot found.\n",
            "dog.944.jpgnot found.\n",
            "dog.945.jpgnot found.\n",
            "dog.946.jpgnot found.\n",
            "dog.947.jpgnot found.\n",
            "dog.948.jpgnot found.\n",
            "dog.949.jpgnot found.\n",
            "dog.950.jpgnot found.\n",
            "dog.951.jpgnot found.\n",
            "dog.952.jpgnot found.\n",
            "dog.953.jpgnot found.\n",
            "dog.954.jpgnot found.\n",
            "dog.955.jpgnot found.\n",
            "dog.956.jpgnot found.\n",
            "dog.957.jpgnot found.\n",
            "dog.958.jpgnot found.\n",
            "dog.959.jpgnot found.\n",
            "dog.960.jpgnot found.\n",
            "dog.961.jpgnot found.\n",
            "dog.962.jpgnot found.\n",
            "dog.963.jpgnot found.\n",
            "dog.964.jpgnot found.\n",
            "dog.965.jpgnot found.\n",
            "dog.966.jpgnot found.\n",
            "dog.967.jpgnot found.\n",
            "dog.968.jpgnot found.\n",
            "dog.969.jpgnot found.\n",
            "dog.970.jpgnot found.\n",
            "dog.971.jpgnot found.\n",
            "dog.972.jpgnot found.\n",
            "dog.973.jpgnot found.\n",
            "dog.974.jpgnot found.\n",
            "dog.975.jpgnot found.\n",
            "dog.976.jpgnot found.\n",
            "dog.977.jpgnot found.\n",
            "dog.978.jpgnot found.\n",
            "dog.979.jpgnot found.\n",
            "dog.980.jpgnot found.\n",
            "dog.981.jpgnot found.\n",
            "dog.982.jpgnot found.\n",
            "dog.983.jpgnot found.\n",
            "dog.984.jpgnot found.\n",
            "dog.985.jpgnot found.\n",
            "dog.986.jpgnot found.\n",
            "dog.987.jpgnot found.\n",
            "dog.988.jpgnot found.\n",
            "dog.989.jpgnot found.\n",
            "dog.990.jpgnot found.\n",
            "dog.991.jpgnot found.\n",
            "dog.992.jpgnot found.\n",
            "dog.993.jpgnot found.\n",
            "dog.994.jpgnot found.\n",
            "dog.995.jpgnot found.\n",
            "dog.996.jpgnot found.\n",
            "dog.997.jpgnot found.\n",
            "dog.998.jpgnot found.\n",
            "dog.999.jpgnot found.\n"
          ]
        }
      ],
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "          try:\n",
        "              shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "          except:\n",
        "                print(fname+\"not found.\")\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77uWcpLxsQGW"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvuGSSysQGW"
      },
      "source": [
        "**Instantiating a small convnet for dogs vs. cats classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "D85XHkEFsQGW"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "UKrblIh4sQGW",
        "outputId": "1334d67c-6eef-44d1-a8a5-517be523c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 178, 178, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 89, 89, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 87, 87, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 43, 43, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 41, 41, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 20, 20, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 18, 18, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 9, 9, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 12545     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 991,041\n",
            "Trainable params: 991,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g1vba2KsQGW"
      },
      "source": [
        "**Configuring the model for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "-xvu687wsQGX"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWGj13MFsQGX"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyNbUhPPsQGX"
      },
      "source": [
        "**Using `image_dataset_from_directory` to read images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TXSWJcImsQGX",
        "outputId": "1b48eee9-1001-4f09-a40a-e98305d95fd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1686 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "iQctpX-nsQGX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "l1_k7ecAsQGY",
        "outputId": "2ff03010-731b-44d6-d1a4-744bda612d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16,)\n",
            "(16,)\n",
            "(16,)\n"
          ]
        }
      ],
      "source": [
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "K0IuyWqwsQGY",
        "outputId": "0c8d5ec6-e7da-4a98-884d-af66d3bd95b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 16)\n",
            "(32, 16)\n",
            "(32, 16)\n"
          ]
        }
      ],
      "source": [
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "l-Tn4msJsQGY",
        "outputId": "2ad67e99-9727-4e38-9d19-1c8c3dc6acc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n",
            "(4, 4)\n",
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshISIHhsQGY"
      },
      "source": [
        "**Displaying the shapes of the data and labels yielded by the `Dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "0pSMLDdOsQGY",
        "outputId": "7f3280cc-0011-4f29-c3dd-31ffa690e97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (32, 180, 180, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QLBxZ_sQGY"
      },
      "source": [
        "**Fitting the model using a `Dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KCYWkCZsQGZ",
        "outputId": "266eb16b-a752-4532-a91e-99aca76373ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "17/53 [========>.....................] - ETA: 1:21 - loss: 1.0377 - accuracy: 0.6066"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REq_LKZsQGZ"
      },
      "source": [
        "**Displaying curves of loss and accuracy during training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l57WjKf0sQGZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WBlpiTlsQGZ"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akSeSfZssQGZ"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJWyG9usQGZ"
      },
      "source": [
        "### Using data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pciat2FQsQGa"
      },
      "source": [
        "**Define a data augmentation stage to add to an image model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWFAUCFrsQGa"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxU7UKdlsQGa"
      },
      "source": [
        "**Displaying some randomly augmented training images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGg6CgZFsQGa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIyIAh3BsQGa"
      },
      "source": [
        "**Defining a new convnet that includes image augmentation and dropout**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65rCuHSvsQGa"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbqZZpc3sQGa"
      },
      "source": [
        "**Training the regularized convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16EdmBqdsQGb"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgrGAjmgsQGb"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PatMKADnsQGb"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOFpu2ZWsQGb"
      },
      "source": [
        "## Leveraging a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbLcg108sQGb"
      },
      "source": [
        "### Feature extraction with a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfHovWyRsQGb"
      },
      "source": [
        "**Instantiating the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2awgfmrNsQGb"
      },
      "outputs": [],
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1YBpiewsQGc"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV8nQroesQGc"
      },
      "source": [
        "#### Fast feature extraction without data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYsYW8ThsQGc"
      },
      "source": [
        "**Extracting the VGG16 features and corresponding labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9c5bSQosQGc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U46w8yczsQGc"
      },
      "outputs": [],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYMAJREqsQGc"
      },
      "source": [
        "**Defining and training the densely connected classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-7dUjxksQGc"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqxgmk7jsQGc"
      },
      "source": [
        "**Plotting the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW34w_AOsQGc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd9fXUkYsQGd"
      },
      "source": [
        "#### Feature extraction together with data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZBF8ApsQGd"
      },
      "source": [
        "**Instantiating and freezing the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOiMKKRIsQGd"
      },
      "outputs": [],
      "source": [
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shTbBCi7sQGd"
      },
      "source": [
        "**Printing the list of trainable weights before and after freezing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt1wASghsQGd"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcvtrc04sQGd"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jue8Zh9LsQGd"
      },
      "source": [
        "**Adding a data augmentation stage and a classifier to the convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlyEfX8IsQGd"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muOrbHnhsQGe"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vkhiFvcsQGe"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSPRJsKasQGe"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoT7QqLwsQGe"
      },
      "source": [
        "### Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujf6mNT-sQGe"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtw1HiUZsQGe"
      },
      "source": [
        "**Freezing all layers until the fourth from the last**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIRyo9V4sQGe"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pou5QkTzsQGe"
      },
      "source": [
        "**Fine-tuning the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKgygqkasQGe"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_zQda5JsQGf"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!7z x train.zip\n"
      ],
      "metadata": {
        "id": "cwwtA3ocsYpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "id": "S2YABoCKzhfH",
        "outputId": "78904c57-b13a-409c-e3e2-8ccedbce5db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "cat-and-dog.zip   kaggle.json\t     sample_data  train\t\ttrain.zip\n",
            "dogs-vs-cats.zip  rebuilt.train.zip  test_set\t  training_set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "! rm -r cats_vs_dogs_small"
      ],
      "metadata": {
        "id": "-ccUmkmQzF0l",
        "outputId": "af726dc2-c798-4128-c5c1-944ba6b9839b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat-and-dog.zip     dogs-vs-cats.zip  rebuilt.train.zip  test_set  training_set\n",
            "cats_vs_dogs_small  kaggle.json       sample_data\t train\t   train.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVJSy3xxsQGf"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter08_intro-to-dl-for-computer-vision.i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}